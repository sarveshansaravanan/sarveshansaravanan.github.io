<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Project 3 — Image Warping and Mosaicing</title>
  <link rel="stylesheet" href="../style.css"/>
  <style>
    .gallery {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 18px;
      justify-items: center;
    }
    figure { margin: 0; text-align: center; }
    figure img { max-width: 100%; height: auto; display: block; border-radius: 6px; }
    header a { text-decoration: none; }
    .section-note { font-size: 0.95rem; opacity: 0.85; }
    pre { background: #f6f8fa; padding: 10px; border-radius: 6px; overflow-x: auto; }
  </style>
</head>
<body>
  <header>
    <a href="../index.html">← Back to Portfolio</a>
    <h1>Project 3 — Image Warping and Mosaicing</h1>
  </header>

  <main>
    <!-- A.1 Shoot and Digitize Pictures -->
    <section id="part1">
      <h2>A.1: Shoot and Digitize Pictures</h2>
      <p class="section-note">
        I captured multiple overlapping photos with a fixed camera center while rotating the camera horizontally to create projective transformations between frames. 
        I ensured roughly 50% overlap between images to make homography estimation stable. Straight lines (like walls and tables) were used to verify minimal distortion.
        Good lighting and static scenes were chosen to prevent ghosting in the final mosaic.
      </p>
      <h4>Input Photos</h4>
      <div class="gallery">
        <figure><img src="media/livingleft.png" alt="Photo 1"><figcaption>Photo 1</figcaption></figure>
        <figure><img src="media/livingright.png" alt="Photo 2"><figcaption>Photo 2</figcaption></figure>
      </div>
      <div class="gallery">
        <figure><img src="media/aptup.png" alt="Photo 1"><figcaption>Photo 1</figcaption></figure>
        <figure><img src="media/aptdown.png" alt="Photo 2"><figcaption>Photo 2</figcaption></figure>
      </div>
      <p class="section-note">These images were taken by fixing the camera at a single point and rotating slightly between shots.</p>
    </section>

    <!-- A.2 Recover Homographies -->
    <section id="part2">
      <h2>A.2: Recover Homographies</h2>
      <p class="section-note">
        To align images, I implemented <code>computeH(im1_pts, im2_pts)</code> to solve for the 3×3 homography matrix H using corresponding points between two images.
        Each correspondence (x, y) ↔ (x', y') generates two linear equations. Stacking all equations leads to an overdetermined system <code>Ah = b</code> solved using least squares.
        This gives 8 unknowns (since H has 9 parameters, but one is fixed to 1). Increasing the number of correspondences improves robustness against noise.
      </p>



      <h4>Example Correspondences</h4>
      <div class="gallery">
        <figure><img src="media/correspondences.png" alt="Correspondences"><figcaption>Selected Correspondences</figcaption></figure>
      </div>
      <div class="gallery">
        <figure><img src="media/visualized.png" alt="Correspondences"><figcaption>Selected Correspondences</figcaption></figure>
      </div>

      <h4>Recovered Homography Matrix</h4>
      <pre><code>
H =
[[    1.3283    -0.0543 -2744.5793]
 [    0.0651     1.0577     5.8011]
 [    0.0001    -0.0001     1.    ]]
      </code></pre>
      
      <p class="section-note">The homography successfully maps points from the first image into alignment with the second.</p>
      <h4>System of Equations</h4>
      <pre><code>
      [x  y  1  0  0  0  -x*x'  -y*x'] [h1]   [x']
      [0  0  0  x  y  1  -x*y'  -y*y'] [h2] = [y']
                                         ...
      </code></pre>
      <p class="section-note">
      Stacking all correspondences yields the linear system <code>A h = b</code>,
      which we solve using least squares (<code>np.linalg.lstsq</code>).
      </p>
      
    
    </section>

    <!-- A.3 Warp the Images -->
    <section id="part3">
      <h2>A.3: Warp the Images</h2>
      <p class="section-note">
        Using the computed homography, I implemented both nearest-neighbor and bilinear interpolation warping methods (inverse warping).
        Bilinear interpolation provided smoother, more visually accurate results, while nearest neighbor was faster but rougher. <h4>Interpolation Trade-offs</h4>
        <p class="section-note">
        Across all images, bilinear interpolation took roughly 2–3× longer than nearest neighbor 
        (e.g., 1.18 s vs 2.54 s for the cheese packet, 0.30 s vs 0.81 s for the door). 
        Nearest neighbor is much faster since it simply rounds to the closest pixel, but the results look blocky and can lose fine detai
        
      </p>


      <p class="section-note">
        <strong>Overall Tradeoff:</strong> Nearest neighbor is computationally cheaper but yields somewhat less smooth results. Bilinear interpolation offers smoother transitions but requires more computation. I used NN because it was faster and you couldn't truly tell much of a difference.
      </p>

      <h3>Rectification Results</h3>
      <p class="section-note">It was easier to use rectangular objects, because we can just use one image, and hand-define a rectangular set of points to compute a homography for.</p>
      <div class="gallery">
        <figure><img src="media/cheese5.png" alt="Rectified Image 1"><figcaption>Original Example 1</figcaption></figure>
        <figure><img src="media/door.png" alt="Rectified Image 2"><figcaption>Original Example 2</figcaption></figure>
      </div>
      <div class="gallery">
        <figure><img src="media/rectifiedcheese.png" alt="Rectified Image 1"><figcaption>Rectified Example 1</figcaption></figure>
        <figure><img src="media/rectifieddoor.png" alt="Rectified Image 2"><figcaption>Rectified Example 2</figcaption></figure>
      </div>
    </section>

    <!-- A.4 Blend Images into Mosaic -->
    <section id="part4">
      <h2>A.4: Blend the Images into a Mosaic</h2>
      <p class="section-note"> The procedure begins with some of the steps from before: choosing matching points and computing thehomography (I used NN for speed). Then, for the final mosaic, I first warped both images into the same coordinate frame using the recovered homography. At first, I simply averaged overlapping pixels, which caused visible seams and black edges. </p> <p class="section-note"> To fix this, I added an alpha mask that fades near image borders. This lets the overlap blend smoothly instead of cutting sharply. Overall, the alpha mask blending removes harsh edges and creates a cleaner mosaic. </p>

      <h4>Source Images</h4>
      <div class="gallery">
        <figure><img src="media/aptup.png" alt="Source 1"><figcaption>Source 1</figcaption></figure>
        <figure><img src="media/aptdown.png" alt="Source 2"><figcaption>Source 2</figcaption></figure>
      </div>
      <div class="gallery">
        <figure><img src="media/halleft.png" alt="Source 1"><figcaption>Source 1</figcaption></figure>
        <figure><img src="media/hallright.png" alt="Source 2"><figcaption>Source 2</figcaption></figure>
      </div>
      <div class="gallery">
        <figure><img src="media/livingleft.png" alt="Source 1"><figcaption>Source 1</figcaption></figure>
        <figure><img src="media/livingright.png" alt="Source 2"><figcaption>Source 2</figcaption></figure>
      </div>

      <h4>Final Mosaics, without proper blending</h4>
      <div class="gallery">
        <figure><img src="media/aptup_aptup_blend.png" alt="Mosaic 1"><figcaption>Mosaic 1</figcaption></figure>
        </div> 
        <div class="gallery">
        <figure><img src="media/livingleft_livingleft_blend.png" alt="Mosaic 2"><figcaption>Mosaic 2</figcaption></figure>
    </div>
        <div class="gallery">
        <figure><img src="media/halleft_halleft_blend.png" alt="Mosaic 3"><figcaption>Mosaic 3</figcaption></figure>
        </div>
        <h4>Final Mosaics after adding mask</h4>
        <p class="section-note">
          Mosaic results after adding masking so that there are no black regions.
        </p>
      <div class="gallery">
        <figure id="apartment-mosaic"><img src="media/aptup_aptup_blend.png" alt="Mosaic 1"><figcaption>Mosaic 1</figcaption></figure>
        </div> 
        <div class="gallery">
        <figure id="livingroom-mosaic"><img src="media/livingleft_livingleft_blend_masked.png" alt="Mosaic 2"><figcaption>Mosaic 2</figcaption></figure>
    </div>
        <div class="gallery">
        <figure id="hallway-mosaic"><img src="media/halleft_halleft_blend_masked.png" alt="Mosaic 3"><figcaption>Mosaic 3</figcaption></figure>
      
    </section>

        <!-- ========================== -->
    <!-- B.1 Harris Corner Detection -->
    <!-- ========================== -->
    <section id="partB1">
      <h2>B.1: Harris Corner Detection</h2>
      <p class="section-note">
        I started by detecting interest points using the Harris Corner Detector. The detector works by computing the corner strenght function: det(H)/trace(H), which 
        essentially picks up on strong gradients in all directions, i.e. a corner.
        The results are then refined with Adaptive Non-Maximal Suppression (ANMS) to not only keep the stronger corners, but also ones that are distributed over the whole image, instead of clustered in a few places.
      </p>

      <h4>Harris Corners (Before ANMS)</h4>
      <div class="gallery">
        <figure><img src="media2/aptup_corners.png" alt="Harris Corners"><figcaption>Detected Corners overlaid on the image</figcaption></figure>
      </div>

      <h4>Adaptive Non-Maximal Suppression (After ANMS)</h4>
      <div class="gallery">
        <figure><img src="media2/aptup_anms.png" alt="ANMS Corners"><figcaption>Chosen Corners after ANMS filtering</figcaption></figure>
      </div>

      <p class="section-note">
        The ANMS-filtered corners are more uniformly distributed across the image, improving spatial coverage for feature matching and alignment.
      </p>
    </section>

    <!-- ================================ -->
    <!-- B.2 Feature Descriptor Extraction -->
    <!-- ================================ -->
    <section id="partB2">
      <h2>B.2: Feature Descriptor Extraction</h2>
      <p class="section-note">
        Using the corners from ANMS, I extract 8×8 descriptors from a 40x40 window around each of the corners and normalize.
      </p>

      <h4>Example Feature Patches from the apartment top image</h4>
      <div class="gallery">
        <figure><img src="media2/feature_patch_1.png" alt="Feature Patch 1"><figcaption>Patch 1</figcaption></figure>
        <figure><img src="media2/feature_patch_2.png" alt="Feature Patch 2"><figcaption>Patch 2</figcaption></figure>
        <figure><img src="media2/feature_patch_3.png" alt="Feature Patch 3"><figcaption>Patch 3</figcaption></figure>
        <figure><img src="media2/feature_patch_4.png" alt="Feature Patch 4"><figcaption>Patch 4</figcaption></figure>
      </div>

      <p class="section-note">
        Each descriptor was sampled and normalized independently to achieve lighting invariance.
      </p>
    </section>

    <!-- ======================= -->
    <!-- B.3 Feature Matching -->
    <!-- ======================= -->
    <section id="partB3">
      <h2>B.3: Feature Matching</h2>
      <p class="section-note">
        After extracting descriptors, I matched features between image pairs by comparing their Euclidean distances. I made use of the Lowe's ratio test.
        This is essentially because we don't want to just choose the strongest matches, but also want to make sure that they're better than the next best match (thresholding this with a ratio). 
        This helps filter out matches where it could go either way; those could lead to incorrect homographies.
      </p>

      <h4>Example Feature Descriptor Matches from the apartment top and bottom images, and actual computed point correspondences</h4>
      <div class="gallery">
        <figure><img src="media2/featurematch.png" alt="Feature match"></figure>
      </div>
      <div class="gallery">
        <figure><img src="media2/pointsmatch.png" alt="Feature match"></figure>
      </div>

      <p class="section-note">
        The resulting correspondences provided candidate pairs for computing robust homographies in the next step using RANSAC.
      </p>
    </section>

    <!-- ======================= -->
    <!-- B.4 RANSAC and Mosaics -->
    <!-- ======================= -->
    <section id="partB4">
      <h2>B.4: RANSAC for Robust Homography</h2>
      <p class="section-note">
        I used 4-point RANSAC to compute the homography from the feature matches. RANSAC (random sample consensus) works how its names sounds: it randomly chooses a sample
        of the correspondes (in this case 4-point) to compute a homography then checks how many corresponds "agree". The homography with the most that agree, "inliers", is then used for the final transformation.
         There is an improvement with the blending for the shell gas station
        mosaic compared to the manual one, so it seems the model was able to compute a better matching than my manual points.
      </p>

      <p class="section-note">
        The three mosaics below link back to their manual mosaics from Part A for comparison.
      </p>

      <h4>Automatic Mosaics (RANSAC-Based)</h4>
      <div class="gallery">
        <figure><a href="#apartment-mosaic"><img src="media2/auto_mosaic_aptup_aptdown.png" alt="Auto Apartment Mosaic"></a><figcaption>Apartment Mosaic (click for manual version)</figcaption></figure>
        <figure><a href="#livingroom-mosaic"><img src="media2/auto_mosaic_livingleft_livingright.png" alt="Auto Living Room Mosaic"></a><figcaption>Living Room Mosaic (click for manual version)</figcaption></figure>
      </div>

      <div class="gallery">
        <figure><a href="#hallway-mosaic"><img src="media2/auto_mosaic_halleft_hallright.png" alt="Auto Hallway Mosaic"></a><figcaption>Hallway Mosaic (click for manual version)</figcaption></figure>
             </div>

      <h4>Manual vs Automatic Comparison</h4>
      <div class="gallery">
        <figure><img src="media2/shelleft.png" alt="Manual"><figcaption>Reference image left</figcaption></figure>
        <figure><img src="media2/shelright.png" alt="Auto"><figcaption>Reference image right</figcaption></figure>
      </div>
      <div class="gallery">
        <figure><img src="media2/shelleft_shelleft_manual.png" alt="Manual Shelf Mosaic"><figcaption>Manual Mosaic</figcaption></figure>
      </div>
      <div class="gallery">
        <figure><img src="media2/auto_mosaic_shelleft_shelright.png" alt="Auto Shelf Mosaic"><figcaption>Automatic Mosaic (RANSAC)</figcaption></figure>
      </div>

      <h4>Additional Automatic Mosaic</h4>
      <div class="gallery">
        <figure><img src="media2/baclleft.png" alt="Manual"><figcaption>Reference image left</figcaption></figure>
        <figure><img src="media2/baclright.png" alt="Auto"><figcaption>Reference image right</figcaption></figure>
      </div>
      <div class="gallery">
        <figure><img src="media2/auto_mosaic_baclleft_baclright.png" alt="Auto Mosaic"><figcaption>Balcony view</figcaption></figure>
      </div>

      <p class="section-note">
        Overall, I learned about how the Harris detector can look for corners by checking for differences in all directions, feature matching by looking for a match significantly better than the other candidates,
      and RANSAC to maximize the amount of inliers for a homography.      </p>
    </section>
    


  </main>
</body>
</html>
